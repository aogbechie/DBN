{
    "collab_server" : "",
    "contents" : "setwd(\"~/sources/bnlearn\")\nlibrary (bnlearn)\nlibrary (Rgraphviz)\nlibrary (devtools)\nlibrary(gRain)\nlibrary (parallel)\nlibrary(R.matlab)\nlibrary(Hmisc)\n\n###################################\n    # 0. Data Preprocessing\n####################################\n\n\n  #define the paths\nroot=\"/home/aogbechie/Desktop/Datasets/Laser/dataset/\"\n#filepaths <- c(\"4272/4272_p1.csv\",\"4272/4272_p2.csv\",\"4272/4272_p3.csv\",\"4272/4272_p4.csv\",\"4276/4276_p1.csv\",\"4276/4276_p2.csv\",\"4276/4276_p3.csv\",\"4276/4276_p4.csv\",\"4275/4275_p1.csv\",\"4275/4275_p2.csv\",\"4275/4275_p3.csv\",\"4275/4275_p4.csv\",\"4274/4274_p1.csv\",\"4274/4274_p2.csv\",\"4274/4274_p3.csv\",\"4274/4274_p4.csv\",\"4273/4273_p1.csv\",\"4273/4273_p2.csv\",\"4273/4273_p3.csv\",\"4273/4273_p4.csv\",\"4271/4271_p1.csv\",\"4271/4271_p2.csv\",\"4271/4271_p3.csv\",\"4271/4271_p4.csv\",\"4270/4270_p1.csv\",\"4270/4270_p2.csv\",\"4270/4270_p3.csv\",\"4270/4270_p4.csv\")\n#filepaths <- c(\"4272/4272_p1.csv\",\"4272/4272_p2.csv\",\"4272/4272_p3.csv\",\"4272/4272_p4.csv\",\"4276/4276_p1.csv\",\"4276/4276_p2.csv\",\"4276/4276_p3.csv\",\"4276/4276_p4.csv\",\"4275/4275_p1.csv\",\"4275/4275_p2.csv\",\"4275/4275_p3.csv\",\"4275/4275_p4.csv\")\nfilepaths <- c(\"4272/4272_p1.csv\",\"4272/4272_p2.csv\",\"4272/4272_p3.csv\",\"4272/4272_p4.csv\")\n#define the pixels to useP\npixels <- sort(c(931,933,934,953,967,981,983,985,987))\n#pixels <- sort(c(321,385, 417, 418,449,450,451, 454, 481, 485, 513, 514, 545, 546, 547, 548, 579, 580, 610, 611, 612, 613, 614, 615, 642, 643, 644, 645, 646, 647)) #30 pixels area izq 321-647\n#pixels <- sort(c(174,321,336,366,385,400,417,418,427,449,450,451,454,456,462,481,483,484,485,495,506,513,514,525,545,546,547,548,560,579,580,592,610,611,612,613,614,615,623,625,626,628,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,661,663,664,665,666,684,692,693,695,696,697,698,717,720,721,778,787,788,790,791,792,815,819,820,821,822,823,825,846,851,852,853,856,880,886,887,901,902,903,904,935,936,941))\nbreaks <- 10 #if continuous set NULL\n#breaks <- NULL #if continuous set NULL\n\n\nfilepaths <- paste(root,filepaths,sep=\"\")\nn_seqs <- length(filepaths)\nn_nodes <- length(pixels)\n\n  #Read the files\nprint(\"Inicio de lectura de ficheros\")\nsystem.time(dataset<-readTimeSerie(filepaths,pixels))\n\nT_length <- length(dataset[[1]][,1])\n  \n  #Discretize the datasets (o no)\nif(! is.null(breaks)){\n  print(\"Inicio de discretizacion de ficheros\")\n  #levels_factor <- as.character(1:breaks)\n  levels_factor <- 1:breaks\n  system.time(dataset<- equalLengthDiscr(dataset, breaks, levels_factor))\n}\n  #Separate the datasets in CPC_0, CPC_t, PC_t and CC_T\nprint(\"Inicio de separaciÃ³n de ficheros\")\nsystem.time({dataset_CPC_0 <- separateDatasetDBN(dataset,\"CPC_0\", breaks);\ndataset_CPC_t <- separateDatasetDBN(dataset,\"CPC_t\", breaks);\ndataset_PC_t <- separateDatasetDBN(dataset,\"PC_t\", breaks);\ndataset_CC_t <- separateDatasetDBN(dataset,\"CC_t\", breaks)})\n\n###################################\n    # 1. DBN Structure Learning\n####################################  \n  #Perform DMMHC optimized in the dataset with dbn[[1]]= Bn0 and dbn[[2]]= dbn (transition model)\n\nprint(\"Inicio de aprendizaje de estructura\")\nif(is.null(breaks)){\n  system.time(dbn_complete <- dmmhc(list(dataset_CPC_0, dataset_CPC_t, dataset_PC_t, dataset_CC_t), test = \"mi-g\", score = \"bic-g\"))\n} else {\n  system.time(dbn_complete <- dmmhc(list(dataset_CPC_0, dataset_CPC_t, dataset_PC_t, dataset_CC_t), test = \"mi\", score = \"bic\"))\n}\n\nbn0 <- dbn_complete[[1]]\nbn_trans <- dbn_complete[[2]]\ngraphviz.plot(bn_trans)\n\n#2-TBN plot\nplot_parameters <- two_tbn.plot(bn_trans, n_nodes)\nplot(plot_parameters[[1]], attrs = plot_parameters[[2]], nodeAttrs=plot_parameters[[3]], subGList = plot_parameters[[4]])\n\n\n###################################\n    # 2. DBN Parameters Learning\n####################################  \n\nbn0.fit <- bn.fit(bn0,dataset_CPC_0, method = \"mle\")\nbn_trans.fit <- bn.fit(bn_trans, dataset_PC_t, method = \"bayes\") #No problem with missing states in instances\n#Convert to gRain\nbn_trans.fit_gRain <- as.grain(bn_trans.fit)\nwrite.net(\"dbn.net\", bn_trans.fit) #writes the hugin file\n\n###################################\n    # 3. Sampling generation\n####################################  \n\n  #Obtain the loglikelihoods of the used sequencies.\nlogliks_samples <- c()\ndataset_sample_PC_t <- list()\nfor(i in 1:length(dataset)){\n  sample_loglik <- 0\n  dataset_sample <- dataset[[i]]\n  dataset_sample_PC_t[[i]] <- separateDatasetDBN(list(dataset_sample),\"PC_t\", breaks)\n  sample_loglik <- logLik(bn_trans.fit, dataset_sample_PC_t[[i]], names(dataset_CPC_t))\n  print(sample_loglik)\n  logliks_samples <- c(logliks_samples, sample_loglik)\n}\n  #Obtain the likelihood of the samples\n\n#Generate Samples\n#options(mc.cores = 1) #Select the number of cores to use if parallel generation\nnum_samples <- T_length \nnum_seqs_sim <- 1\nprint(\"Inicio de sampling\")\n#system.time(samples <- sample_dbn(bn_trans.fit_gRain,num_seqs_sim,num_samples))\nsamp<- c(num_samples,num_seqs_sim)\ndbnet <- as.bnet(bn_trans.fit, bn_trans$arcs)\n\n#Compute Loglikelihood\nfor(i in 1:length(samples)){\n  dataset_sample<- samples[[i]]\n  dataset_sample_PC_t <- int2factor_breaks(dataset_sample, breaks)\n  complete_score <- score(bn_trans,dataset_sample_PC_t, type = \"loglik\")\n  sample_loglik <- loglik.dbn(dataset_sample_PC_t[,-(1:n_nodes)], complete_score, score = \"loglik\")\n  logliks_samples <- c(logliks_samples, sample_loglik)\n}\n###################################\n    # 4. Experiment for each sequence\n####################################  \n\n#Alpha test\nalpha <- 5 #in percentege\nindex <- round(length(logliks_samples)*alpha/100)\nif(index < 1){\n  index <-1\n}\nthreshold <- sort(logliks_samples)[index]\n\n\n#Sequence preparation\nexp_filepath <- c(\"4274/4274_p2.csv\")\nexp_filepath <- paste(root,exp_filepath,sep=\"\")\nexp_dataset <- readTimeSerie(exp_filepath,pixels)\nexp_dataset <- equalLengthDiscr(exp_dataset,breaks, levels_factor)\nexp_dataset_PC_t <- separateDatasetDBN(exp_dataset,\"PC_t\", breaks)\n#Loglikelihood calculation of the unseen sequence\nsystem.time(\n  complete_score <- score(bn_trans,exp_dataset_PC_t, type = \"loglik\"),\n  exp_loglik <- loglik.dbn(exp_dataset_PC_t[,-(1:n_nodes)], complete_score, score = \"loglik\"))\n# Check if sequence is normal or not\nif(exp_loglik < threshold){\n  print(\"The sequence contains anomallies\")\n} else {\n  print(\"The sequence is normal\")\n}\n\n\n\n",
    "created" : 1457601507687.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2541447242",
    "id" : "DF39220B",
    "lastKnownWriteTime" : 1458747588,
    "last_content_update" : 1458747588595,
    "path" : "~/sources/bnlearn/R_beto/dmmhc.R",
    "project_path" : "R_beto/dmmhc.R",
    "properties" : {
        "source_window_id" : ""
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}